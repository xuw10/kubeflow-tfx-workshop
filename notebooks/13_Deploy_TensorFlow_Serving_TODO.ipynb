{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "# Train and serve a TensorFlow model with TensorFlow Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbVhjPpzn6BM"
   },
   "source": [
    "This guide trains a neural network model to classify [images of clothing, like sneakers and shirts](https://github.com/zalandoresearch/fashion-mnist), saves the trained model, and then serves it with [TensorFlow Serving](https://www.tensorflow.org/serving/).  The focus is on TensorFlow Serving, rather than the modeling and training in TensorFlow, so for a complete example which focuses on the modeling and training see the [Basic Classification example](https://www.tensorflow.org/tutorials/keras/basic_classification).\n",
    "\n",
    "This guide uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dzLKpmZICaWN",
    "outputId": "da36b678-5ff6-4e2c-dd9e-2dc3170bba8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jAk1ZXqTJqN"
   },
   "source": [
    "## Create your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yR0EdgrLCaWR"
   },
   "source": [
    "### Import the Fashion MNIST dataset\n",
    "\n",
    "This guide uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "Fashion MNIST is intended as a drop-in replacement for the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset—often used as the \"Hello, World\" of machine learning programs for computer vision. You can access the Fashion MNIST directly from TensorFlow, just import and load the data.\n",
    "\n",
    "Note: Although these are really images, they are loaded as NumPy arrays and not binary image objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "7MqDQO0KCaWS",
    "outputId": "18ee19db-cc5e-4141-9e27-ba8f52b2c148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "40960/29515 [=========================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "26435584/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "4431872/4422102 [==============================] - 0s 0us/step\n",
      "\n",
      "train_images.shape: (60000, 28, 28, 1), of float64\n",
      "test_images.shape: (10000, 28, 28, 1), of float64\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# scale the values to 0.0 to 1.0\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# reshape for feeding into the model\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print('\\ntrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype))\n",
    "print('test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PDu7OX8Nf5PY"
   },
   "source": [
    "### Train and evaluate your model\n",
    "\n",
    "Let's use the simplest possible CNN, since we're not focused on the modeling part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "LTNN0ANGgA36",
    "outputId": "9733b5ce-820f-43e6-de52-7bef7bcb759d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1 (Conv2D)               (None, 13, 13, 8)         80        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1352)              0         \n",
      "_________________________________________________________________\n",
      "Softmax (Dense)              (None, 10)                13530     \n",
      "=================================================================\n",
      "Total params: 13,610\n",
      "Trainable params: 13,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 10s 172us/sample - loss: 0.5269 - acc: 0.8176\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 10s 162us/sample - loss: 0.3827 - acc: 0.8656\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 9s 145us/sample - loss: 0.3459 - acc: 0.8770\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 10s 166us/sample - loss: 0.3249 - acc: 0.8841\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 10s 167us/sample - loss: 0.3113 - acc: 0.8884\n",
      "10000/10000 [==============================] - 1s 90us/sample - loss: 0.3467 - acc: 0.8772\n",
      "\n",
      "Test accuracy: 0.877200007439\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "  keras.layers.Conv2D(input_shape=(28,28,1), filters=8, kernel_size=3, \n",
    "                      strides=2, activation='relu', name='Conv1'),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10, activation=tf.nn.softmax, name='Softmax')\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "testing = False\n",
    "epochs = 5\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=epochs)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('\\nTest accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AwGPItyphqXT"
   },
   "source": [
    "## Save your model\n",
    "\n",
    "To load our trained model into TensorFlow Serving we first need to save it in [SavedModel](https://www.tensorflow.org/api_docs/python/tf/saved_model) format.  This will create a protobuf file in a well-defined directory hierarchy, and will include a version number.  TensorFlow Serving allows us to select [which version of a model, or \"servable\"](../overview) we want to use when we make inference requests.  Each version will be exported to a different sub-directory under the given path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "0w5Rq8SsgWE6",
    "outputId": "0c318fa8-4368-4b9d-d7c6-d9d35b8f6aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export_path = /var/tmp/1\n",
      "\n",
      "\n",
      "Saved model:\n",
      "total 120\n",
      "-rw-r--r-- 1 root root 118459 Jun 13 21:57 saved_model.pb\n",
      "drwxr-xr-x 2 root root   4096 Jun 13 21:57 variables\n"
     ]
    }
   ],
   "source": [
    "# Fetch the Keras session and save the model\n",
    "# The signature definition is defined by the input and output tensors,\n",
    "# and stored with the default serving key\n",
    "import tempfile\n",
    "\n",
    "MODEL_DIR = tempfile.gettempdir()\n",
    "version = 1\n",
    "export_path = os.path.join(MODEL_DIR, str(version))\n",
    "print('export_path = {}\\n'.format(export_path))\n",
    "if os.path.isdir(export_path):\n",
    "  print('\\nAlready saved a model, cleaning up\\n')\n",
    "  !rm -r {export_path}\n",
    "\n",
    "tf.saved_model.simple_save(\n",
    "    keras.backend.get_session(),\n",
    "    export_path,\n",
    "    inputs={'input_image': model.input},\n",
    "    outputs={t.name:t for t in model.outputs})\n",
    "\n",
    "print('\\nSaved model:')\n",
    "!ls -l {export_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FM7B_RuDYoIj"
   },
   "source": [
    "## Examine your saved model\n",
    "\n",
    "We'll use the command line utility `saved_model_cli` to look at the [MetaGraphDefs](https://www.tensorflow.org/api_docs/python/tf/MetaGraphDef) (the models) and [SignatureDefs](../signature_defs) (the methods you can call) in our SavedModel.  See [this discussion of the SavedModel CLI](https://www.tensorflow.org/guide/saved_model#cli_to_inspect_and_execute_savedmodel) in the TensorFlow Guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "LU4GDF_aYtfQ",
    "outputId": "51a3405c-4bd0-4c0d-833e-168ef7b669ac",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n",
      "\r\n",
      "signature_def['serving_default']:\r\n",
      "  The given SavedModel SignatureDef contains the following input(s):\r\n",
      "    inputs['input_image'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 28, 28, 1)\r\n",
      "        name: Conv1_input:0\r\n",
      "  The given SavedModel SignatureDef contains the following output(s):\r\n",
      "    outputs['Softmax/Softmax:0'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 10)\r\n",
      "        name: Softmax/Softmax:0\r\n",
      "  Method name is: tensorflow/serving/predict\r\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {export_path} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following does not work within this notebook environment..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSPWuegUb7Eo"
   },
   "source": [
    "That tells us a lot about our model!  In this case we just trained our model, so we already know the inputs and outputs, but if we didn't this would be important information.  It doesn't tell us everything, like the fact that this is grayscale image data for example, but it's a great start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DBgsyhytS6KD"
   },
   "source": [
    "## Serve your model with TensorFlow Serving\n",
    "\n",
    "### Add TensorFlow Serving distribution URI as a package source:\n",
    "\n",
    "We're preparing to install TensorFlow Serving using [Aptitude](https://wiki.debian.org/Aptitude) since this notebook runs in a Debian environment.  We'll add the `tensorflow-model-server` package to the list of packages that Aptitude knows about.  Note that we're running as root.\n",
    "\n",
    "Note: This example is running TensorFlow Serving natively, but [you can also run it in a Docker container](../docker), which is one of the easiest ways to get started using TensorFlow Serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "EWg9X2QHlbGS",
    "outputId": "d3d8416d-dee5-48d3-cbcc-288acc210a46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2343  100  2343    0     0  90115      0 --:--:-- --:--:-- --:--:-- 90115\n",
      "mktemp: failed to create directory via template ‘/tmp/apt-key-gpghome.XXXXXXXXXX’: No such file or directory\n",
      "Get:1 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease [3,012 B]\n",
      "Err:1 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease    \u001b[0m\u001b[33m\n",
      "  Couldn't create temporary file /tmp/apt.conf.uNx521 for passing config to apt-key\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease                        \u001b[0m\u001b[33m\n",
      "Err:3 http://archive.ubuntu.com/ubuntu bionic InRelease                        \u001b[0m\n",
      "  Couldn't create temporary file /tmp/apt.conf.L9XIV7 for passing config to apt-key\n",
      "Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]    \n",
      "Get:4 https://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6,372 B][0m\n",
      "Err:4 https://packages.cloud.google.com/apt cloud-sdk-bionic InRelease         \u001b[0m\n",
      "  Couldn't create temporary file /tmp/apt.conf.eGS3k9 for passing config to apt-key\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]      \u001b[33m\n",
      "Err:2 http://security.ubuntu.com/ubuntu bionic-security InRelease0m  \u001b[0m\u001b[33m\n",
      "  Couldn't create temporary file /tmp/apt.conf.xwJ82j for passing config to apt-key\n",
      "Err:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\u001b[33m\n",
      "  Couldn't create temporary file /tmp/apt.conf.O3TQQo for passing config to apt-key\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Err:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
      "  Couldn't create temporary file /tmp/apt.conf.NvGl8s for passing config to apt-key\n",
      "Reading package lists... Error!\n",
      "W: GPG error: http://storage.googleapis.com/tensorflow-serving-apt stable InRelease: Couldn't create temporary file /tmp/apt.conf.uNx521 for passing config to apt-key\n",
      "E: The repository 'http://storage.googleapis.com/tensorflow-serving-apt stable InRelease' is not signed.\n",
      "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
      "N: See apt-secure(8) manpage for repository creation and user configuration details.\n",
      "W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: http://archive.ubuntu.com/ubuntu bionic InRelease: Couldn't create temporary file /tmp/apt.conf.L9XIV7 for passing config to apt-key\n",
      "W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://packages.cloud.google.com/apt cloud-sdk-bionic InRelease: Couldn't create temporary file /tmp/apt.conf.eGS3k9 for passing config to apt-key\n",
      "W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: http://security.ubuntu.com/ubuntu bionic-security InRelease: Couldn't create temporary file /tmp/apt.conf.xwJ82j for passing config to apt-key\n",
      "W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: http://archive.ubuntu.com/ubuntu bionic-updates InRelease: Couldn't create temporary file /tmp/apt.conf.O3TQQo for passing config to apt-key\n",
      "W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: http://archive.ubuntu.com/ubuntu bionic-backports InRelease: Couldn't create temporary file /tmp/apt.conf.NvGl8s for passing config to apt-key\n",
      "E: Sub-process returned an error code\n",
      "E: Couldn't create temporary file to work with /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_bionic_InRelease - mkstemp (2: No such file or directory)\n",
      "E: The package lists or status file could not be parsed or opened.\n"
     ]
    }
   ],
   "source": [
    "# This is the same as you would do from your command line, but without the [arch=amd64], and no sudo\n",
    "# You would instead do:\n",
    "# echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
    "# curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\n",
    "\n",
    "!sudo echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
    "    curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
    "!apt update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1ZVp_VOU7Wu"
   },
   "source": [
    "### Install TensorFlow Serving\n",
    "\n",
    "This is all you need - one command line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "ygwa9AgRloYy",
    "outputId": "485b7d6c-68a4-4ff6-9690-830fc1cd2ba6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Error!\n",
      "E: Couldn't create temporary file to work with /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_bionic_InRelease - mkstemp (2: No such file or directory)\n",
      "E: The package lists or status file could not be parsed or opened.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install tensorflow-model-server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k5NrYdQeVm52"
   },
   "source": [
    "### Start running TensorFlow Serving\n",
    "\n",
    "This is where we start running TensorFlow Serving and load our model.  After it loads we can start making inference requests using REST.  There are some important parameters:\n",
    "\n",
    "* `rest_api_port`: The port that you'll use for REST requests.\n",
    "* `model_name`: You'll use this in the URL of REST requests.  It can be anything.\n",
    "* `model_base_path`: This is the path to the directory where you've saved your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aUgp3vUdU5GS"
   },
   "outputs": [],
   "source": [
    "os.environ[\"MODEL_DIR\"] = MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kJDhHNJVnaLN",
    "outputId": "3dc18177-7c73-4943-e177-5b65400ec453"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job # 2 in a separate thread.\n"
     ]
    }
   ],
   "source": [
    "%%bash --bg \n",
    "nohup tensorflow_model_server \\\n",
    "  --rest_api_port=8501 \\\n",
    "  --model_name=fashion_model \\\n",
    "  --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "IxbeiOCUUs2z",
    "outputId": "648d5262-e5c5-4b78-cbcf-b31c06b33581",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: failed to run command 'tensorflow_model_server': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!tail server.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwg1JKaGXWAg"
   },
   "source": [
    "## Make a request to your model in TensorFlow Serving\n",
    "\n",
    "First, let's take a look at a random example from our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "id": "Luqm_Jyff9iR",
    "outputId": "85e6d60f-3755-4d36-f87d-ad9aaba4ab52"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show(idx, title):\n",
    "  plt.figure()\n",
    "  plt.imshow(test_images[idx].reshape(28,28))\n",
    "  plt.axis('off')\n",
    "  plt.title('\\n\\n{}'.format(title), fontdict={'size': 16})\n",
    "\n",
    "import random\n",
    "rando = random.randint(0,len(test_images)-1)\n",
    "show(rando, 'An Example Image: {}'.format(class_names[test_labels[rando]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TKnEHeTrbh3L"
   },
   "source": [
    "Ok, that looks interesting.  How hard is that for you to recognize? Now let's create the JSON object for a batch of  three inference requests, and see how well our model recognizes things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2dsD7KQG1m-R",
    "outputId": "c56e8d06-b1d3-4528-9ac5-860c2091dd7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: {\"instances\": [[[[0.0], [0.0], [0.0], [0.0], [0.0] ... 0.0], [0.0]]]], \"signature_name\": \"serving_default\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": test_images[0:3].tolist()})\n",
    "print('Data: {} ... {}'.format(data[:50], data[len(data)-52:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ReQd4QESIwXN"
   },
   "source": [
    "### Make REST requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iT3J-lHrhOYQ"
   },
   "source": [
    "#### Newest version of the servable\n",
    "\n",
    "We'll send a predict request as a POST to our server's REST endpoint, and pass it three examples.  We'll ask our server to give us the latest version of our servable by not specifying a particular version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "id": "vGvFyuIzW6n6",
    "outputId": "f731f319-8390-4df9-909c-7cffa56d7fcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 19.0.1, however version 19.1.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=8501): Max retries exceeded with url: /v1/models/fashion_model:predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f5e62148890>: Failed to establish a new connection: [Errno 111] Connection refused',))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-01aa9d4ca65c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"content-type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://localhost:8501/v1/models/fashion_model:predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \"\"\"\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py2/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=8501): Max retries exceeded with url: /v1/models/fashion_model:predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f5e62148890>: Failed to establish a new connection: [Errno 111] Connection refused',))"
     ]
    }
   ],
   "source": [
    "!pip install -q requests\n",
    "\n",
    "import requests\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post('http://localhost:8501/v1/models/fashion_model:predict', data=data, headers=headers)\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "\n",
    "show(0, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n",
    "  class_names[np.argmax(predictions[0])], test_labels[0], class_names[np.argmax(predictions[0])], test_labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJH8LtM4XELp"
   },
   "source": [
    "#### A particular version of the servable\n",
    "\n",
    "Now let's specify a particular version of our servable.  Since we only have one, let's select version 1.  We'll also look at all three results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 920
    },
    "colab_type": "code",
    "id": "zRftRxeR1tZx",
    "outputId": "c04d6187-9f67-40df-b43b-08819bc20110"
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=8501): Max retries exceeded with url: /v1/models/fashion_model/versions/1:predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f5e621486d0>: Failed to establish a new connection: [Errno 111] Connection refused',))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d53509ea9e51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"content-type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://localhost:8501/v1/models/fashion_model/versions/1:predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \"\"\"\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py2/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=8501): Max retries exceeded with url: /v1/models/fashion_model/versions/1:predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f5e621486d0>: Failed to establish a new connection: [Errno 111] Connection refused',))"
     ]
    }
   ],
   "source": [
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post('http://localhost:8501/v1/models/fashion_model/versions/1:predict', data=data, headers=headers)\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "\n",
    "for i in range(0,3):\n",
    "  show(i, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n",
    "    class_names[np.argmax(predictions[i])], test_labels[i], class_names[np.argmax(predictions[i])], test_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "rest_simple.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
